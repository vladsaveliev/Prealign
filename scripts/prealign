#!/usr/bin/env python
import sys
py_v = sys.version_info[:2]
if not (py_v == (2, 7) or py_v >= (3, 3)):
    sys.exit('Only Python 2.7 or 3.3 and up are supported. Current version: ' + '.'.join(py_v))

import os
from optparse import OptionParser, SUPPRESS_HELP
from os.path import join, isfile, basename, isdir, exists, dirname, splitext, islink, realpath, relpath, abspath
from collections import OrderedDict, defaultdict
import time
import subprocess
import traceback

from ngs_utils.proc_args import set_up_dirs
from ngs_utils.bed_utils import verify_bed
from ngs_utils.call_process import run
from ngs_utils import logger
from ngs_utils.file_utils import verify_dir, verify_file, safe_mkdir, adjust_path, which, file_transaction, can_reuse
from ngs_utils.logger import critical, debug, info, send_email, err, warn
from ngs_utils.utils import is_az, is_us, is_cluster, is_local
from ngs_utils.parallel import ParallelCfg, parallel_view
from ngs_utils.parallel import parallel_view
import ngs_utils.reference_data as ref

import targqc

import az
from az.jira_utils import retrieve_jira_info
from az.webserver.exposing import sync_with_ngs_server, convert_gpfs_path_to_url

from prealign.dataset_structure import DatasetStructure

from ngs_reporting import version

TOOL_NAME = 'prealign'


class Steps:
    fastqc = True
    targqc = True
    samtools_stats = False
    metamapping = False
    expose = True


options = [
    (['--test'], dict(
        dest='test',
        help='Quick test of correct installation.'
    )),
    (['--bed'], dict(
        dest='bed',
        help='BED file with regions to analyse. If not specified, CDS across full genome will be analysed',
    )),
    (['-o', '--analysis-dir'], dict(
        dest='analysis_dir',
        metavar='DIR',
        help='Analysis directory for reports. If there are multiple subprojects, use --conf'
     )),
    (['--project-name'], dict(
        dest='project_name',
        help=SUPPRESS_HELP,
    )),
    (['-j', '--jira'], dict(
        dest='jira',
        help='JIRA case path (goes to the NGS website)',
    )),
    (['--conf'], dict(
        dest='hiseq4000_conf',
        help='A csv-file in the form of "subproject_dir_name,analysis_path,jira_url,target_bed_path"',
    )),
    (['--samplesheet'], dict(
        dest='samplesheet',
        help='Sample sheet (default is located under the dataset root as SampleSheet.csv',
    )),
    (['--email'], dict(
        dest='email',
        help='E-mail address to send notifications on errors and finished jobs.',
    )),
    (['--expose-only'], dict(
        dest='expose_to_ngs_server_only',
        action='store_true',
        default=False,
        help='Only add project to the webserver',
    )),
    (['--no-expose'], dict(
        dest='expose',
        action='store_false',
        default=True,
        help='Do not expose the reports',
    )),
    (['--no-targqc'], dict(
        dest='targqc',
        action='store_false',
        default=True,
        help='',
    )),
    (['--metamapping'], dict(
        dest='metamapping',
        action='store_true',
        default=False,
        help='',
    )),
    (['--no-fastqc'], dict(
        dest='fastqc',
        action='store_false',
        default=True,
        help='',
    )),
    (['-g', '--genome'], dict(
        dest='genome',
        help='Genome build (hg19 or hg38), used to pick genome annotation BED file if not specified',
        default='hg19',
    )),
    (['-t', '--nt', '--threads'], dict(
        dest='threads',
        type='int',
        help='Number of threads. Default is in corresponding system_info_*.yaml or 1. '
             'If set to 1, skip starting cluster even if scheduler is specified.'
     )),
    (['-s', '--scheduler'], dict(
        dest='scheduler',
        choices=["lsf", "sge", "torque", "slurm", "pbspro"],
        help="Scheduler to use for ipython parallel"
     )),
    (['-q', '--queue'], dict(
        dest='queue',
        help="Scheduler queue to run jobs on, for ipython parallel"
     )),
    (['-r', '--resources'], dict(
        dest='resources',
        help=('Cluster specific resources specifications. Can be specified multiple times.\n'
              'Supports SGE, Torque, LSF and SLURM parameters.'),
        default=[],
        action="append"
     )),
    (['--no-dedup'], dict(
        dest='no_dedup',
        action='store_true',
        default=not az.dedup,
        help=SUPPRESS_HELP,
     )),
    (['--debug'], dict(
        dest='debug',
        action='store_true',
        default=logger.debug,
        help=SUPPRESS_HELP,
     )),
    (['--work-dir'], dict(dest='work_dir', metavar='DIR', help=SUPPRESS_HELP)),
    (['--log-dir'], dict(dest='log_dir', metavar='DIR', help=SUPPRESS_HELP)),
]


def proc_opts():
    usage = 'Usage: %prog <DATASET_DIR_LOCATION> <JIRA_URL(S)>' \
            ' [--bed BED] [--project-name STR]]\n' \
            '\tNote that DATASET_DIR_LOCATION can be either the root of a Illumina run ' \
            '(in witch case multiple projects may be detected and processed, or ' \
            'a specific project in <root>/Unalign/<Project>'
    parser = OptionParser(description='Pre-process and QC Illumina run directory')
    parser.set_usage(usage)
    for args, kwargs in options:
        parser.add_option(*args, **kwargs)
    opts, args = parser.parse_args()

    if opts.work_dir:
        opts.debug = True
    logger.init(opts.debug)

    # Reading inputs
    if len(args) < 1:
        critical(usage)

    # /ngs/oncology/datasets/hiseq/150521_D00443_0159_AHK2KTADXX or subproject
    input_dir = verify_dir(args[0], is_critical=True, description='Dataset directory')

    if opts.hiseq4000_conf and opts.analysis_dir:
        critical('If used with hiseq4000 muilti-subproject conf file, -o cannot be used: please, specify the '
                 'output locations in the conf file.')

    if not opts.hiseq4000_conf:
        if not opts.analysis_dir and not opts.project_name:
            warn('Warning: neither output dir with -o nor the project name with --project were specified')

    # TODO: if hiseq4000_conf is set??
    if opts.analysis_dir:
        analysis_dir = adjust_path(opts.analysis_dir)
        output_dir = join(analysis_dir, TOOL_NAME)
    else:
        analysis_dir = None
        output_dir = join(input_dir, TOOL_NAME)

    output_dir, work_dir, log_dir = set_up_dirs(TOOL_NAME, output_dir=output_dir, work_dir=opts.work_dir,
                                                log_dir=opts.log_dir)
    # if opts.analysis_dir (-o):
    #    - analysis_dir = <opts.analysis_dir>
    #    - output_dir   = <opts.analysis_dir>/prealign
    #    - work_dir     = <opts.analysis_dir>/prealign/work_dir
    # else (or hiseq4000_conf):
    #    - analysis_dir = None
    #    - output_dir   = <dataset_dir>/prealign
    #    - work_dir     = <dataset_dir>/prealign/work_dir

    try:
        subprocess.call(['chmod', '-R', '775', work_dir])
    except OSError:
        debug(traceback.format_exc())
        pass

    project_name = opts.project_name
    if not project_name and analysis_dir:
        project_name = basename(realpath(analysis_dir))
    if project_name:
        info('Project name: ' + project_name)

    samplesheet = None
    if opts.samplesheet:
        info('Using custom sample sheet ' + opts.samplesheet)
        samplesheet = verify_file(opts.samplesheet, is_critical=True)

    bed_fpath = None
    if opts.bed:
        bed_fpath = verify_bed(opts.bed, 'BED', is_critical=True)
        debug('Using BED ' + bed_fpath)

    hiseq4000_conf = None
    if opts.hiseq4000_conf:
        hiseq4000_conf = verify_file(opts.hiseq4000_conf, 'HiSeq4000 conf file', is_critical=True)
        debug('Using conf file for HiSeq4000 run: ' + hiseq4000_conf)

    if opts.expose_to_ngs_server_only:
        Steps.fastqc = Steps.metamapping = Steps.targqc = False
    else:
        Steps.fastqc = opts.fastqc
        Steps.metamapping = opts.metamapping
        Steps.targqc = opts.targqc
        Steps.expose = opts.expose

    # Parallel configuration and genomes; TODO: make it nicer; probably use "cnf"-like class like before, and not import it
    sys_cfg = az.init_sys_cfg()
    sys_cfg['scheduler'] = opts.scheduler or sys_cfg.get('scheduler')
    sys_cfg['queue']     = opts.queue     or sys_cfg.get('queue')
    sys_cfg['resources'] = opts.resources or sys_cfg.get('resources')
    sys_cfg['threads']   = opts.threads   or sys_cfg.get('threads') or 1
    tag = ('prealign_' + project_name) if project_name else 'prealign'
    parallel_cfg = ParallelCfg(sys_cfg['scheduler'], sys_cfg['queue'],
                               sys_cfg['resources'], sys_cfg['threads'], tag)

    logger.set_smtp_host(sys_cfg.get('smtp_host'))

    return output_dir, hiseq4000_conf, analysis_dir, project_name, opts.jira, bed_fpath, \
           input_dir, work_dir, samplesheet, parallel_cfg, opts.genome


def read_hiseq4000_conf(conf_fpath):
    proj_infos = dict()
    if verify_file(conf_fpath, is_critical=True, description='HiSeq4000 subproject configuration file'):
        with open(conf_fpath) as f:
            for i, l in enumerate(f):
                l = l.strip('\n')
                if not l.startswith('#'):
                    fs = l.split(',')
                    if not len(fs) == 4:
                        critical('A line in ' + conf_fpath + ' should contain 4 comma-separated values (empty allowed) '
                                 '(ds_proj_name,analysis_dir_or_project_name,jira,bed_path). Malformed line #' + str(i) + ': ' + l)
                    p = ProjInfo(ds_proj_name=fs[0])
                    if fs[1]:
                        if fs[1].startswith('/'):
                            p.analysis_dir = adjust_path(fs[1])
                            info('Assuming ' + fs[1] + ' is a output path')
                        else:
                            p.project_name = fs[1]
                            info('Assuming ' + fs[1] + ' is a project name')
                    p.jira = fs[2]
                    p.bed = fs[3]
                    proj_infos[p.ds_proj_name] = p
    if len(proj_infos) == 0:
        critical('No records in ' + conf_fpath)
    return proj_infos


def parse_jira_case(jira_url):
    jira_case = None
    if is_az() and jira_url:
        info('Getting info from JIRA...')
        jira_case = retrieve_jira_info(jira_url)
    return jira_case


class ProjInfo:
    def __init__(self, ds_proj_name, output_dir=None, analysis_dir=None,
                 project_name=None, jira=None, bed=None):
        self.ds_proj_name = ds_proj_name
        self.output_dir = output_dir
        self.analysis_dir = analysis_dir
        self.project_name = project_name
        self.jira = jira
        self.bed = bed


def _read_projects_infos(output_dir, hiseq4000_conf,
                         analysis_dir, project_name, jira_url, bed_fpath):
    # TODO: work if analysis_dir or project_name is not specified

    if hiseq4000_conf:
        proj_infos = read_hiseq4000_conf(hiseq4000_conf)
    else:
        proj_infos = {'': ProjInfo(
            ds_proj_name='',
            output_dir=output_dir,
            analysis_dir=analysis_dir,
            project_name=project_name,
            jira=jira_url,
            bed=bed_fpath
        )}
    for pi in proj_infos.values():
        if not pi.project_name and pi.analysis_dir:
            pi.project_name = basename(realpath(pi.analysis_dir))
        if not pi.analysis_dir and pi.project_name:
            pi.analysis_dir = join(output_dir, pi.project_name)
        if not pi.output_dir:
            if pi.analysis_dir:
                pi.output_dir = join(pi.analysis_dir, TOOL_NAME)  # join(output_dir, pi.project_name, 'prealign')
            else:
                pi.output_dir = join(output_dir, '{ds_proj_name}')
            # else set as output_dir/ds_project_name inside DatasetStructure.create
    if hiseq4000_conf:
        # symlinking the analysis dir paths provided in the conf, into output_dir
        for pi in proj_infos.values():
            if pi.analysis_dir:
                symlink = join(output_dir, pi.project_name)
                if not exists(symlink) and not islink(symlink):
                    try:
                        os.symlink(pi.analysis_dir, symlink)
                    except OSError:
                        debug('Cannot symlink analysis dir paths provided in the conf, into output_dir: ' +
                              pi.analysis_dir + ' -> ' + symlink)

    # jira_case_by_subprj = dict()
    # for proj in proj_infos.values():
    #     jira_case_by_subprj[proj.project_name] = parse_jira_case(jira_url)
    #     if not output_dir_by_subproject[subprj] and jira_case_by_subprj[subprj]:
    #         prjname_by_subprj[subprj] = jira_case_by_subprj[subprj].project_name
    #         info('Setting project name from JIRA: ' + prjname_by_subprj[subprj])
    #     if not prjname_by_subprj[subprj]:
    #         critical('Cannot parse JIRA url ' + str(jira_url) +
    #            ', and project name is not specified in the config or command line. Please, provide a project name.')
    #     prjname_by_subprj[subprj] = prjname_by_subprj[subprj].replace(' ', '_')
    #     info('Project name and JIRA URL for "' + subprj + '": ' + prjname_by_subprj[subprj] + ', ' + str(jira_url))

    return proj_infos


def _prepare_analysis_dirs(input_dir, proj_infos, samplesheet, hiseq4000_conf, work_dir):
    ds = DatasetStructure.create(input_dir, proj_infos, samplesheet)
    # TODO: make work without az_prjname_by_subprj
    if not ds.project_by_name:
        critical('Error: no projects found')
    info('Projects: ' + ', '.join([p.name + ' (' + ', '.join(p.sample_by_name) + ')'
                                   for p in ds.project_by_name.values()]))
    for project in ds.project_by_name.values():
        if project.output_dir:
            safe_mkdir(project.output_dir)

        if not project.sample_by_name:
            critical('Error: no samples for project ' + project.name + ' found')
        if len(ds.project_by_name) > 1 and project.name not in proj_infos:
            critical('Error: ' + project.name + 'could not be found in config ' + str(hiseq4000_conf))

    for project in ds.project_by_name.values():
        # Creating analysis directory
        samples = project.sample_by_name.values()
        if project.analysis_dir:
            __prepare_analysis_dir(
                safe_mkdir(join(work_dir, project.name)), project.analysis_dir,
                project.ds_dir, project.az_project_name, samples)
    return ds


def main():
    output_dir, hiseq4000_conf, analysis_dir, project_name, jira_url, bed_fpath, \
        input_dir, work_dir, samplesheet, parallel_cfg, genome = proc_opts()

    proj_infos = _read_projects_infos(output_dir, hiseq4000_conf,
                                      analysis_dir, project_name, jira_url, bed_fpath)
    info()
    info('*' * 60)
    ds = _prepare_analysis_dirs(input_dir, proj_infos, samplesheet, hiseq4000_conf, work_dir)

    _run_pipeline(ds, proj_infos, work_dir, parallel_cfg, genome)


def _run_pipeline(ds, proj_infos, work_dir, parallel_cfg, genome):
    info('Preparing fastq files')
    for project in ds.project_by_name.values():
        project.concat_fastqs(ds.get_fastq_regexp_fn)

    read_pairs_num_by_sample_by_proj = defaultdict(dict)

    info('Downsampling and aligning reads')
    bwa_prefix = az.get_refdata(genome)['bwa']
    if not bwa_prefix:
        critical('--bwa-prefix is required when running from fastq')
    for project in ds.project_by_name.values():
        samples = project.sample_by_name.values()
        with parallel_view(len(samples), parallel_cfg, join(work_dir, 'sge_fastq')) as view:
            targqc.proc_fastq(
                samples, view, work_dir, bwa_prefix,
                downsample_to=float(az.downsample_fraction),
                num_pairs_by_sample=read_pairs_num_by_sample_by_proj[project.name],
                dedup=az.dedup)

    # if Steps.targqc:
    #     info('Running TargQC for downsampled reads')
    #     for project in ds.project_by_name.values():
    #         samples = project.sample_by_name.values()
    #         tq_samples = [targqc.Sample(s.name,
    #             dirpath=safe_mkdir(join(project.downsample_targqc_dirpath, s.name)),
    #             work_dir=safe_mkdir(join(work_dir, s.name)), bam=s.bam) for s in samples]
    #         bed_fpath = list(proj_infos.values())[0].bed
    #         if project.name in proj_infos:
    #             bed_fpath = proj_infos[project.name].bed
    #         run_targqc(safe_mkdir(join(work_dir, project.name)), project, tq_samples, bed_fpath,
    #                    read_pairs_num_by_sample_by_proj[project.name], parallel_cfg, genome)

    if Steps.fastqc:
        for project in ds.project_by_name.values():
            samples = project.sample_by_name.values()
            info('Making FastQC reports')
            safe_mkdir(project.fastqc_dirpath)
            make_fastqc_reports(safe_mkdir(join(work_dir, project.name)), samples, project.fastqc_dirpath, parallel_cfg)
            for s in samples:
                if s.l_fqc_sample:
                    read_pairs_num_by_sample_by_proj[project.name][s.name] = \
                        get_read_pairs_num_from_fastqc(s.l_fqc_sample.fastqc_txt_fpath)

        # if Steps.samtools_stats:
        #     info()
        #     info('Running SamTools stats')
        #     # TODO
        #     safe_mkdir(ds.samtools_stats)
        #     run_samtools_stats(samples, project.downsample_metamapping_dirpath)

        # Making project-level report
        # make_project_level_report(cnf, dataset_structure=ds, dataset_project=project)
    for project in ds.project_by_name.values():
        info()
        info('*' * 70)
        info('Making MultiQC report')
        __make_multiqc(safe_mkdir(join(work_dir, project.name)), ds, project)

    for project in ds.project_by_name.values():
        samples = project.sample_by_name.values()
        if (is_az() or is_local()) and Steps.expose:
            info()
            info('Syncing with the NGS webserver')
            jira = list(proj_infos.values())[0].jira
            if project.name in proj_infos:
                jira = proj_infos[project.name].jira

            html_report_url = sync_with_ngs_server(
                safe_mkdir(join(work_dir, project.name)),
                jira_url=jira,
                project_name=project.az_project_name or project.name,
                sample_names=[s.name for s in samples],
                preproc_dirpath=project.output_dir,
                summary_report_fpath=project.multiqc_report_html_fpath,
            )
        else:
            html_report_url = convert_gpfs_path_to_url(project.multiqc_report_html_fpath)

        subj = project.name
        txt = 'Preproc finished for ' + str(project.az_project_name) + '\n'
        txt += '\n'
        txt += 'Datasets path: ' + str(project.ds_dir) + '\n'
        if project.analysis_dir:
            txt += 'Analysis path: ' + str(project.analysis_dir) + '\n'
        txt += 'Report: ' + str(html_report_url) + '\n'
        if jira_url:
            txt += 'Jira: ' + jira_url
        send_email(txt, subj)

        info()
        info('Finished processing project ' + project.name)
        info('  Dataset location:')
        info('    ' + project.ds_dir)
        info('  Merged FastQ location:')
        info('    ' + project.fastq_dirpath)
        if project.analysis_dir:
            info('  Analysis location:')
            info('    ' + project.analysis_dir)
        info('  Report:')
        if html_report_url:
            info('    ' + html_report_url)
        else:
            info('    ' + project.multiqc_report_html_fpath)

    # if not cnf.debug and cnf.work_dir:
    #     try:
    #         shutil.rmtree(cnf.work_dir)
    #     except OSError:
    #         err('Can\'t remove work directory ' + cnf.work_dir + ', please, remove it manually.')


def __make_multiqc(work_dir, ds, project):
    cmd = 'multiqc -v -f'
    to_run = None

    if project.downsample_targqc_dirpath and isdir(project.downsample_targqc_dirpath):
        cmd += ' ' + project.downsample_targqc_dirpath
        to_run = True
    if project.fastqc_dirpath and isdir(project.fastqc_dirpath):
        cmd += ' ' + project.fastqc_dirpath
        to_run = True
    if ds.basecalls_reports_dirpath and isdir(ds.basecalls_reports_dirpath):
        cmd += ' ' + ds.basecalls_reports_dirpath
        to_run = True
    if to_run:
        cmd += ' -o ' + project.output_dir

        metadata_dict = dict(project_name=project.name or basename(ds.illumina_dir))
        run_info_dict = dict()
        run_info_dict['run_date'] = time.strftime('%d %b %Y, %H:%M (GMT%z)', time.localtime())
        if version.__version__:
            run_info_dict['suite_version'] = 'Reporting Suite v.' + version.__version__
        run_info_dict['analysis_dir'] = project.output_dir
        metadata_dict['run_section'] = run_info_dict

        metadata_fpath = join(work_dir, 'az_multiqc_metadata.yaml')
        import yaml
        with open(metadata_fpath, 'w') as outfile:
            yaml.dump(metadata_dict, outfile, default_flow_style=False)
        metadata_fpath = metadata_fpath.replace('.yaml', '.json')
        import json
        with open(metadata_fpath, 'w') as outfile:
            json.dump(metadata_dict, outfile)
        cmd += ' --az-metadata ' + metadata_fpath
        cmd += ' -m targqc -m fastqc -m bcl2fastq'
        run(cmd)
        verify_file(project.multiqc_report_html_fpath, is_critical=True,
                    description='MultiQC report for ' + project.name)


def get_read_pairs_num_from_fastqc(l_fastqc_txt_fpath):
    num_reads = 0
    with open(l_fastqc_txt_fpath) as f_in:
        for line in f_in:
            if 'total sequences' in line.lower():
                num_reads += int(line.strip().split('\t')[-1])
                break
    return num_reads


def run_targqc(work_dir, project, samples, bed_fpath, read_pairs_num_by_sample, parallel_cfg, genome):
    targqc_work_dir = safe_mkdir(join(work_dir, 'targqc'))

    # target = Target(work_dir, cfg.fai_fpath, cfg.reuse_intermediate, bed_fpath)
    # info()
    # start_targqc(work_dir, samples, target)
    targqc.start_targqc(targqc_work_dir, project.downsample_targqc_dirpath, samples, bed_fpath,
                        parallel_cfg,
                        az.get_refdata(genome)['bwa'],
                        fai_fpath=ref.get_fai(genome),
                        genome=genome,
                        dedup=az.dedup,
                        num_pairs_by_sample=read_pairs_num_by_sample)

    # cmdl = (('targqc ' + ' '.join(s.l_fpath + ' ' + s.r_fpath for s in samples) +
    #     ' --work-dir {targqc_work_dir} --project-name {project.name} ' +
    #     ' -o {project.downsample_targqc_dirpath} --genome {genome} --bwa-prefix {bwa}').format(**locals()))
    # if bed_fpath:
    #     cmdl += ' --bed ' + bed_fpath
    # safe_mkdir(project.downsample_targqc_dirpath)
    # if cfg.reuse_intermediate:
    #     cmdl += ' --reuse'
    # if cfg.parallel_cfg.scheduler:
    #     cmdl += ' --scheduler ' + cfg.parallel_cfg.scheduler
    # if cfg.parallel_cfg.queue:
    #     cmdl += ' --queue ' + cfg.parallel_cfg.queue
    # if cfg.parallel_cfg.threads:
    #     cmdl += ' -t ' + str(cfg.parallel_cfg.threads)
    # if cfg.parallel_cfg.extra_params:
    #     cmdl += ' '.join((' --resources ' + k + '=' + v) for k, v in cfg.parallel_cfg.extra_params.items())

    # run(cmdl)

    verify_file(project.downsample_targqc_report_fpath, is_critical=True)
    return project.downsample_targqc_report_fpath

    # samples = [TargQCSample(
    #     s.name,
    #     output_dir=join(targqc_dirpath, s.name),
    #     bed=cnf.bed,
    #     bam=bam_by_sample[s.name])
    #            for s in samples]

    # Parallel(n_jobs=threads)(delayed(make_targetseq_reports(
    #     CallCnf(cnf.__dict__), sample.dirpath, sample,
    #     sample.bam, exons_bed, exons_no_genes_bed, target_bed
    # )(CallCnf(cnf.__dict__), sample) for sample in samples))
    #
    # return summarize_targqc(cnf, 1, targqc_dirpath, samples, bed_fpath, exons_bed)


def run_fastqc(work_dir, fastq_fpath, output_basename, fastqc_dirpath):
    from ngs_utils.file_utils import verify_file, safe_mkdir, which, can_reuse
    from ngs_utils.logger import debug
    from ngs_utils.call_process import run
    from os.path import join, isfile
    fastqc = which('fastqc')
    java = which('java')
    tmp_dirpath = join(work_dir, 'FastQC_' + output_basename + '_tmp')
    safe_mkdir(tmp_dirpath)
    fastq_html_fpath = join(fastqc_dirpath, output_basename + '_fastqc', 'fastqc_report.html')
    if can_reuse(fastq_html_fpath, fastq_fpath):
        debug(fastq_html_fpath + ' exists, reusing')
        return fastq_html_fpath
    cmdline_l = '{fastqc} --dir {tmp_dirpath} --extract -o {fastqc_dirpath} -f fastq -j {java} {fastq_fpath}'.format(**locals())
    run(cmdline_l)
    return verify_file(fastq_html_fpath, 'FastQC html report')


# def run_samtools_stats():
#     """Run samtools stats with reports on mapped reads, duplicates and insert sizes.
#     """
#     stats_file = os.path.join(out_dir, "%s.txt" % dd.get_sample_name(data))
#     if not utils.file_exists(stats_file):
#         utils.safe_makedir(out_dir)
#         samtools = config_utils.get_program("samtools", data["config"])
#         with file_transaction(data, stats_file) as tx_out_file:
#             cmd = "{samtools} stats {bam_file}"
#             cmd += " > {tx_out_file}"
#             do.run(cmd.format(**locals()), "samtools stats", data)
#     out = _parse_samtools_stats(stats_file)
#     out.update(_parse_offtargets(bam_file))
#     return out


def find_fastq_pairs_by_sample_names(fastq_fpaths, sample_names):
    fastq_by_sn = OrderedDict()

    for sn in sample_names:
        sn_fastq_fpaths = sorted([f for f in fastq_fpaths if basename(f).startswith(sn + '_R')])
        if len(sn_fastq_fpaths) == 0:
            err('Error: no fastq found for ' + sn)
            fastq_by_sn[sn] = None
        elif len(sn_fastq_fpaths) > 2:
            critical('Error: more than 2 fastq files starting with ' + sn + '_R: ' + ', '.join(sn_fastq_fpaths))
        elif len(sn_fastq_fpaths) == 1:
            warn('Warning: only single fastq file is found for ' + sn + '. Treating as single reads.')
            fastq_by_sn[sn] = [verify_file(sn_fastq_fpaths[0], description='sn_fastq_fpaths[0] for ' + str(sn)), None]
        else:
            fastq_by_sn[sn] = [verify_file(fpath, description='fpath from sn_fastq_fpaths for ' + str(sn)) for fpath in sn_fastq_fpaths]

    return fastq_by_sn


class FQC_Sample:
    def __init__(self, fqc_sample_name, fastq_fpath, sample):
        self.name = fqc_sample_name
        self.fastq_fpath = fastq_fpath
        self.sample = sample
        self.fastqc_html_fpath = self.find_fastqc_html()
        self.fastqc_dirpath = None
        self.fastqc_txt_fpath = None

    def find_fastqc_html(self):
        sample_fastqc_dirpath = join(self.sample.fastqc_dirpath, self.name + '_fastqc')
        fastqc_html_fpath = join(self.sample.fastqc_dirpath, self.name + '_fastqc.html')
        if isfile(fastqc_html_fpath):
            return fastqc_html_fpath
        else:
            fastqc_html_fpath = join(sample_fastqc_dirpath, 'fastqc_report.html')
            if isfile(fastqc_html_fpath):
                return fastqc_html_fpath
            else:
                return None


def make_fastqc_reports(work_dir, samples, fastqc_dirpath, parallel_cfg):
    fastqc = which('fastqc')
    if not fastqc:
        err('"fastqc" executable is not found, cannot make reports')
        return None

    else:
        safe_mkdir(fastqc_dirpath)

        fqc_samples = []
        for s in samples:
            s.l_fqc_sample = FQC_Sample(fqc_sample_name=s.l_fastqc_base_name, fastq_fpath=s.l_fpath, sample=s)
            s.r_fqc_sample = FQC_Sample(fqc_sample_name=s.r_fastqc_base_name, fastq_fpath=s.r_fpath, sample=s)
            fqc_samples.extend([s.l_fqc_sample, s.r_fqc_sample])

        with parallel_view(len(fqc_samples), parallel_cfg, work_dir) as view:
            fastq_reports_fpaths = view.run(run_fastqc, [
                [work_dir, fqc_s.fastq_fpath, fqc_s.name, fastqc_dirpath] for fqc_s in fqc_samples])

        for fqc_s, fqc_html_fpath in zip(fqc_samples, fastq_reports_fpaths):
            if not fqc_html_fpath or not verify_file(fqc_html_fpath):
                err('FastQC report for ' + fqc_s.name + ' ' + str(fqc_html_fpath) + ' not found')
            else:
                fqc_s.fastqc_html_fpath = fqc_html_fpath

            fqc_s.fastqc_dirpath = join(fastqc_dirpath, fqc_s.name + '_fastqc')
            if not verify_dir(fqc_s.fastqc_dirpath):
                err('FastQC results directory for ' + fqc_s.name + ' ' + fqc_s.fastqc_dirpath + ' not found')
                fqc_s.fastqc_dirpath = None

            if isfile(fqc_s.fastqc_dirpath + '.zip'):
                try:
                    os.remove(fqc_s.fastqc_dirpath + '.zip')
                except OSError:
                    pass

            fastqc_txt_fpath = join(fqc_s.fastqc_dirpath, 'fastqc_data.txt')
            if not verify_file(fastqc_txt_fpath):
                err('FastQC txt for ' + fqc_s.name + ' ' + fastqc_txt_fpath + ' not found')
            fqc_s.fastqc_txt_fpath = fastqc_txt_fpath


def __prepare_analysis_dir(work_dir, analysis_dir, ds_project_dir, az_project_name, samples):
    safe_mkdir(analysis_dir)
    bcbio_csv_fpath = join(analysis_dir, 'bcbio.csv')
    if not isfile(bcbio_csv_fpath):
        with file_transaction(work_dir, bcbio_csv_fpath) as tx:
            with open(tx, 'w') as f:
                f.write('samplename,description,batch,phenotype\n')
                for s in samples:
                    f.write(s.name + ',' + s.name + ',' + s.name + '-batch,tumor\n')

    # TODO: symlink outside of prealign subdir
    # simlink analysis -> dataset
    symlink_in_output_to_ds = join(analysis_dir, 'dataset')
    if isdir(analysis_dir) and isdir(ds_project_dir) and not islink(symlink_in_output_to_ds) and not exists(symlink_in_output_to_ds):
        info('Creating symlink ' + basename(symlink_in_output_to_ds) + ' in analysis ' +
             dirname(analysis_dir) + ' pointing to datasets: ' + ds_project_dir)
        os.symlink(ds_project_dir, symlink_in_output_to_ds)

    # simlink dataset -> analysis
    symlink_in_ds_to_output = join(ds_project_dir, az_project_name)
    if not exists(symlink_in_ds_to_output) and not islink(symlink_in_ds_to_output):
        info('Creating symlink in Datasets subproject location (' + ds_project_dir + ') called ' +
             basename(symlink_in_ds_to_output) + ' pointing to ' + analysis_dir)
        try:
            os.symlink(realpath(analysis_dir), symlink_in_ds_to_output)
        except OSError as e:
            info('Could not create symlink to Analysis in Datasets (' + str(e) + '), skipping.')


if __name__ == '__main__':
    main()


'''
#!/bin/bash/

#Takes 2 arguements, data_loc and project_name, as created in datasets, such as hiseq and Dev_0200_HiSeq_DS
#Usage - upload_seqQC.sh hiseq Dev_0200_HiSeq_DS https://jira.rd.astrazeneca.net/browse/NGSG-313
#Usage - upload_seqQC.sh bioscience Bio_0041_IDT_RR_DS

datasets=/ngs/oncology/datasets
data_loc=$1
project_name=$2

cd /opt/lampp/htdocs/seqQC/
#echo "In /opt/lampp/htdocs/seqQC on NGS Server"
echo " "

mkdir $project_name
cd $project_name
mkdir FastQC

echo "Demultiplex Report linked!"
echo " "

echo "SampleSheet linked!"
echo "DONE!"
echo " "

'''


